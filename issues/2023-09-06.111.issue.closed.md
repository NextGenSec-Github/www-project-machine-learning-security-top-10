# [\#111 Issue](https://github.com/OWASP/www-project-machine-learning-security-top-10/issues/111) `closed`: Implementing demo for ML10:2023 Model Poisoning

#### <img src="https://avatars.githubusercontent.com/u/59761275?u=4e5c4c8a6242383369a6696ab3e21d1df9c875d1&v=4" width="50">[aryanxk02](https://github.com/aryanxk02) opened issue at [2023-09-06 20:30](https://github.com/OWASP/www-project-machine-learning-security-top-10/issues/111):

Thinking of [ML10:2023 Model Poisoning](https://owasp.org/www-project-machine-learning-security-top-10/docs/ML10_2023-Model_Poisoning.html), we can create two scripts that, although carrying out the same operation (perhaps classification), which provide different outcomes. 

By this was, we can showcase model poisoning in action along with the theory corresponding to it.

Please share your ideas with me on this!

cc: @sagarbhure @shsingh @robvanderveer 




-------------------------------------------------------------------------------



[Export of Github issue for [OWASP/www-project-machine-learning-security-top-10](https://github.com/OWASP/www-project-machine-learning-security-top-10).]
